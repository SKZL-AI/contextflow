# ContextFlow AI - Docker Compose Configuration
# Usage: docker-compose up -d

version: "3.9"

services:
  # ===========================================================================
  # ContextFlow UI (React/Vite Frontend)
  # ===========================================================================
  ui:
    build:
      context: ./ui
      dockerfile: Dockerfile
      target: ${UI_BUILD_TARGET:-production}
    image: contextflow-ui:latest
    container_name: contextflow-ui
    restart: unless-stopped
    ports:
      - "${CONTEXTFLOW_UI_PORT:-80}:80"
    environment:
      - VITE_API_URL=${VITE_API_URL:-http://localhost:8000}
      - VITE_API_WS_URL=${VITE_API_WS_URL:-ws://localhost:8000}
      - NODE_ENV=${NODE_ENV:-production}
    depends_on:
      contextflow:
        condition: service_healthy
    networks:
      - contextflow-network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:80/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # ===========================================================================
  # ContextFlow UI Development (Hot Reload)
  # ===========================================================================
  ui-dev:
    build:
      context: ./ui
      dockerfile: Dockerfile
      target: development
    image: contextflow-ui:dev
    container_name: contextflow-ui-dev
    restart: unless-stopped
    ports:
      - "${CONTEXTFLOW_UI_DEV_PORT:-5173}:5173"
    environment:
      - VITE_API_URL=${VITE_API_URL:-http://localhost:8000}
      - VITE_API_WS_URL=${VITE_API_WS_URL:-ws://localhost:8000}
      - NODE_ENV=development
    volumes:
      # Mount source for hot reload
      - ./ui/src:/app/src:ro
      - ./ui/public:/app/public:ro
      - ./ui/index.html:/app/index.html:ro
      - ./ui/vite.config.ts:/app/vite.config.ts:ro
    depends_on:
      contextflow:
        condition: service_healthy
    networks:
      - contextflow-network
    profiles:
      - dev

  # ===========================================================================
  # ContextFlow API Server
  # ===========================================================================
  contextflow:
    build:
      context: .
      dockerfile: Dockerfile
    image: contextflow:latest
    container_name: contextflow-api
    restart: unless-stopped
    ports:
      - "${CONTEXTFLOW_API_PORT:-8000}:8000"
    environment:
      # LLM Provider API Keys (from .env)
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY:-}
      - GROQ_API_KEY=${GROQ_API_KEY:-}
      - MISTRAL_API_KEY=${MISTRAL_API_KEY:-}
      # Local providers
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://host.docker.internal:11434}
      - VLLM_BASE_URL=${VLLM_BASE_URL:-http://host.docker.internal:8001}
      # ContextFlow settings
      - CONTEXTFLOW_DEFAULT_PROVIDER=${CONTEXTFLOW_DEFAULT_PROVIDER:-claude}
      - CONTEXTFLOW_LOG_LEVEL=${CONTEXTFLOW_LOG_LEVEL:-INFO}
      - CONTEXTFLOW_ENABLE_CORS=true
      - CONTEXTFLOW_CORS_ORIGINS=${CONTEXTFLOW_CORS_ORIGINS:-*}
    volumes:
      # Persist session data
      - contextflow-data:/app/data
      # Logs
      - contextflow-logs:/app/logs
    healthcheck:
      test: ["CMD", "python", "-c", "import httpx; httpx.get('http://localhost:8000/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - contextflow-network
    # Uncomment to use GPU (requires nvidia-docker)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  # ===========================================================================
  # Redis (Optional - for distributed sessions/caching)
  # ===========================================================================
  redis:
    image: redis:7-alpine
    container_name: contextflow-redis
    restart: unless-stopped
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - contextflow-network
    profiles:
      - with-redis

  # ===========================================================================
  # Ollama (Optional - for local LLM inference)
  # ===========================================================================
  ollama:
    image: ollama/ollama:latest
    container_name: contextflow-ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama-models:/root/.ollama
    # Uncomment for GPU support
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]
    networks:
      - contextflow-network
    profiles:
      - with-ollama

# =============================================================================
# Networks
# =============================================================================
networks:
  contextflow-network:
    driver: bridge

# =============================================================================
# Volumes
# =============================================================================
volumes:
  contextflow-data:
    name: contextflow-data
  contextflow-logs:
    name: contextflow-logs
  redis-data:
    name: contextflow-redis-data
  ollama-models:
    name: contextflow-ollama-models

# =============================================================================
# Usage Examples
# =============================================================================
# Basic (API + Production UI):
#   docker-compose up -d
#
# Development mode (with hot reload UI on port 5173):
#   docker-compose --profile dev up -d
#
# With Redis:
#   docker-compose --profile with-redis up -d
#
# With Ollama:
#   docker-compose --profile with-ollama up -d
#
# All services (production UI):
#   docker-compose --profile with-redis --profile with-ollama up -d
#
# All services (development UI):
#   docker-compose --profile dev --profile with-redis --profile with-ollama up -d
#
# View logs:
#   docker-compose logs -f contextflow
#   docker-compose logs -f ui
#   docker-compose logs -f ui-dev
#
# Stop:
#   docker-compose down
#
# Stop and remove volumes:
#   docker-compose down -v
#
# Rebuild UI after changes:
#   docker-compose build ui
#   docker-compose up -d ui
